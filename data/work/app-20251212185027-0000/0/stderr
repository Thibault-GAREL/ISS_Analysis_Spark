Spark Executor Command: "/opt/java/openjdk/bin/java" "-cp" "/opt/spark/conf:/opt/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=36865" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@spark-master:36865" "--executor-id" "0" "--hostname" "172.19.0.3" "--cores" "2" "--app-id" "app-20251212185027-0000" "--worker-url" "spark://Worker@172.19.0.3:46521" "--resourceProfileId" "0"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/12/12 18:50:29 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 115@dce5e25619dd
25/12/12 18:50:29 INFO SignalUtils: Registering signal handler for TERM
25/12/12 18:50:29 INFO SignalUtils: Registering signal handler for HUP
25/12/12 18:50:29 INFO SignalUtils: Registering signal handler for INT
25/12/12 18:50:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/12 18:50:30 INFO SecurityManager: Changing view acls to: 185
25/12/12 18:50:30 INFO SecurityManager: Changing modify acls to: 185
25/12/12 18:50:30 INFO SecurityManager: Changing view acls groups to: 
25/12/12 18:50:30 INFO SecurityManager: Changing modify acls groups to: 
25/12/12 18:50:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: 185; groups with view permissions: EMPTY; users with modify permissions: 185; groups with modify permissions: EMPTY
25/12/12 18:50:30 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.2:36865 after 93 ms (0 ms spent in bootstraps)
25/12/12 18:50:31 INFO SecurityManager: Changing view acls to: 185
25/12/12 18:50:31 INFO SecurityManager: Changing modify acls to: 185
25/12/12 18:50:31 INFO SecurityManager: Changing view acls groups to: 
25/12/12 18:50:31 INFO SecurityManager: Changing modify acls groups to: 
25/12/12 18:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: 185; groups with view permissions: EMPTY; users with modify permissions: 185; groups with modify permissions: EMPTY
25/12/12 18:50:31 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.2:36865 after 4 ms (0 ms spent in bootstraps)
25/12/12 18:50:31 INFO DiskBlockManager: Created local directory at /tmp/spark-d976d357-cc0a-4aca-870a-5d67ac64b949/executor-76fcfe8f-3cbe-4b22-8ac6-c4ff689337af/blockmgr-6901b334-ec2b-42e8-97d5-f03ab4233b74
25/12/12 18:50:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/12/12 18:50:31 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@spark-master:36865
25/12/12 18:50:31 INFO WorkerWatcher: Connecting to worker spark://Worker@172.19.0.3:46521
25/12/12 18:50:31 INFO TransportClientFactory: Successfully created connection to /172.19.0.3:46521 after 4 ms (0 ms spent in bootstraps)
25/12/12 18:50:31 INFO WorkerWatcher: Successfully connected to spark://Worker@172.19.0.3:46521
25/12/12 18:50:31 INFO ResourceUtils: ==============================================================
25/12/12 18:50:31 INFO ResourceUtils: No custom resources configured for spark.executor.
25/12/12 18:50:31 INFO ResourceUtils: ==============================================================
25/12/12 18:50:31 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
25/12/12 18:50:31 INFO Executor: Starting executor ID 0 on host 172.19.0.3
25/12/12 18:50:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34513.
25/12/12 18:50:31 INFO NettyBlockTransferService: Server created on 172.19.0.3:34513
25/12/12 18:50:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/12/12 18:50:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.19.0.3, 34513, None)
25/12/12 18:50:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.19.0.3, 34513, None)
25/12/12 18:50:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.19.0.3, 34513, None)
25/12/12 18:50:31 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/12/12 18:50:46 INFO CoarseGrainedExecutorBackend: Got assigned task 0
25/12/12 18:50:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/12/12 18:50:46 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
25/12/12 18:50:46 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.2:35589 after 4 ms (0 ms spent in bootstraps)
25/12/12 18:50:46 INFO CoarseGrainedExecutorBackend: Got assigned task 3
25/12/12 18:50:46 INFO Executor: Running task 1.0 in stage 3.0 (TID 3)
25/12/12 18:50:46 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
25/12/12 18:50:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2038.0 B, free 434.3 MiB)
25/12/12 18:50:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 119.8 KiB, free 434.3 MiB)
25/12/12 18:50:46 INFO TorrentBroadcast: Reading broadcast variable 2 took 378 ms
25/12/12 18:50:46 INFO TorrentBroadcast: Reading broadcast variable 7 took 144 ms
25/12/12 18:50:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.4 KiB, free 434.0 MiB)
25/12/12 18:50:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 335.1 KiB, free 434.0 MiB)
25/12/12 18:50:46 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
25/12/12 18:50:46 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
25/12/12 18:50:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1295 bytes result sent to driver
25/12/12 18:50:47 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
25/12/12 18:50:47 INFO TransportClientFactory: Successfully created connection to /172.19.0.4:39021 after 3 ms (0 ms spent in bootstraps)
25/12/12 18:50:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.9 MiB)
25/12/12 18:50:47 INFO TorrentBroadcast: Reading broadcast variable 6 took 69 ms
25/12/12 18:50:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 375.6 KiB, free 433.5 MiB)
25/12/12 18:50:47 INFO StateStore: State Store maintenance task started
25/12/12 18:50:47 INFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@424a26ce
25/12/12 18:50:47 INFO StateStore: Reported that the loaded instance StateStoreProviderId(StateStoreId(file:/opt/spark-data/checkpoints/iss_statistics/state,0,1,default),4149e154-23e4-49c8-8ee6-c9a3654567ed) is active
25/12/12 18:50:47 WARN HDFSBackedStateStoreProvider: The state for version 79 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read snapshot file for version 70 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/70.snapshot
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 71 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/71.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 72 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/72.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 73 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/73.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 74 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/74.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 75 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/75.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 76 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/76.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 77 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/77.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 78 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/78.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Read delta file for version 79 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] from file:/opt/spark-data/checkpoints/iss_statistics/state/0/1/79.delta
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Retrieved version 79 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] for update
25/12/12 18:50:48 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
25/12/12 18:50:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 433.5 MiB)
25/12/12 18:50:48 INFO TorrentBroadcast: Reading broadcast variable 5 took 15 ms
25/12/12 18:50:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 375.6 KiB, free 433.1 MiB)
25/12/12 18:50:48 INFO StateStore: Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef@424a26ce
25/12/12 18:50:48 INFO StateStore: Reported that the loaded instance StateStoreProviderId(StateStoreId(file:/opt/spark-data/checkpoints/iss_statistics/state,0,1,default),4149e154-23e4-49c8-8ee6-c9a3654567ed) is active
25/12/12 18:50:48 INFO HDFSBackedStateStoreProvider: Retrieved version 79 of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/spark-data/checkpoints/iss_statistics/state/0/1] for readonly
25/12/12 18:50:48 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
25/12/12 18:50:48 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@spark-master:36865)
25/12/12 18:50:48 INFO MapOutputTrackerWorker: Got the map output locations
25/12/12 18:50:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/12/12 18:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
25/12/12 18:50:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
25/12/12 18:50:48 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
